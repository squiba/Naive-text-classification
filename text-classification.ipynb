{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification ( Naive Bayes )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "20newsgroup dataset will be downloaded and divided in training(60%) and test(40%) dataset. [source code for fetching the dataset](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/datasets/twenty_newsgroups.py) (see the fetch_20newgroup function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of examples: 18846\n",
      "training examples: 11314\n",
      "test examples: 7532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='all',shuffle=True,random_state=42,data_home='scikit_learn_data/')\n",
    "twenty_train = fetch_20newsgroups(subset='train',shuffle=True,random_state=42,data_home='scikit_learn_data/')\n",
    "twenty_test = fetch_20newsgroups(subset='test',shuffle=True,random_state=42,data_home='scikit_learn_data/')\n",
    "\n",
    "print(\"no of examples:\", len(dataset.data))\n",
    "print(\"training examples:\",len(twenty_train.data))\n",
    "print(\"test examples:\", len(twenty_test.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Features from text data\n",
    "Two different type of features will be used. [source code for feature extraction](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py) (see the CounVectorizer and TfidfTransformer class)  \n",
    "For each example\n",
    "1. count vectorizer = word_id: no of words present\n",
    "2. tfidf transformer = word_id: tfidf value\n",
    "\n",
    "Note:  \n",
    "1. fit_transform: fit acording to the data provided and than tranform text into vector as mentioned above\n",
    "2. transform: transorm the data into vector as already fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# tokenizing the text\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_test_counts = count_vect.transform(twenty_test.data)\n",
    "\n",
    "# calculating tfidf (Term_frequency * inverse_document_frequency)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the Naive Bayes Model\n",
    "[Naive Bayes model](http://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes) with \n",
    "1. smoothing prior alpha =1(in each document add one instance of every word) and \n",
    "2. fit_prior = false ( initial probability of classes will be taken equal)  \n",
    "[source code for naive bayes](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/naive_bayes.py) (see the MultinomialNB class)\n",
    "\n",
    "Two models are generated for different typr of features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf_naive_count = MultinomialNB(alpha=1,fit_prior='false').fit(X_train_counts, twenty_train.target)\n",
    "clf_naive_tfidf = MultinomialNB(alpha=1,fit_prior='false').fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model\n",
    "saving the trained model on the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trained_models/naive_tfidf/Naive_tfidf.pkl',\n",
       " 'trained_models/naive_tfidf/Naive_tfidf.pkl_01.npy',\n",
       " 'trained_models/naive_tfidf/Naive_tfidf.pkl_02.npy',\n",
       " 'trained_models/naive_tfidf/Naive_tfidf.pkl_03.npy',\n",
       " 'trained_models/naive_tfidf/Naive_tfidf.pkl_04.npy',\n",
       " 'trained_models/naive_tfidf/Naive_tfidf.pkl_05.npy']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ckeck for directory and make them\n",
    "import os\n",
    "naive_count_dir = \"trained_models/naive_count\"\n",
    "naive_tfidf_dir = \"trained_models/naive_tfidf\"\n",
    "naive_count_path = os.path.join(naive_count_dir,'Naive_count.pkl')\n",
    "naive_tfidf_path = os.path.join(naive_tfidf_dir,'Naive_tfidf.pkl')\n",
    "if not os.path.exists(naive_count_dir):\n",
    "    os.makedirs(naive_count_dir)\n",
    "if not os.path.exists(naive_tfidf_dir):\n",
    "    os.makedirs(naive_tfidf_dir)\n",
    "    \n",
    "# save the model\n",
    "import joblib\n",
    "joblib.dump(clf_naive_count,naive_count_path)\n",
    "joblib.dump(clf_naive_tfidf,naive_tfidf_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the class of an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism :       9.69249730763e-07\n",
      "comp.graphics :       0.00157018218978\n",
      "comp.os.ms-windows.misc :       2.72851955035e-08\n",
      "comp.sys.ibm.pc.hardware :       0.0248204115856\n",
      "comp.sys.mac.hardware :       0.221292944274\n",
      "comp.windows.x :       2.60102082051e-06\n",
      "misc.forsale :       0.695596318096\n",
      "rec.autos :       0.0031841460353\n",
      "rec.motorcycles :       0.0216113398158\n",
      "rec.sport.baseball :       9.36946876277e-06\n",
      "rec.sport.hockey :       8.56827655908e-06\n",
      "sci.crypt :       0.0185170680563\n",
      "sci.electronics :       0.0107097204999\n",
      "sci.med :       0.000185816821492\n",
      "sci.space :       4.83882839364e-05\n",
      "soc.religion.christian :       0.000108325618836\n",
      "talk.politics.guns :       0.000326284499462\n",
      "talk.politics.mideast :       1.59245425214e-05\n",
      "talk.politics.misc :       0.00195445763946\n",
      "talk.religion.misc :       3.71367412366e-05\n",
      "\n",
      "category:  misc.forsale\n"
     ]
    }
   ],
   "source": [
    "test_example = [\"my laptop is fantastic.buy it today and get free offer.best cheap quality discount\"]\n",
    "test_example_count = count_vect.transform(test_example)\n",
    "probs = clf_naive_count.predict_proba(test_example_count)[0] # alternate: predict_log_proba\n",
    "\n",
    "for i in range(len(twenty_train.target_names)):\n",
    "    print(twenty_train.target_names[i],\":      \",probs[i])\n",
    "    \n",
    "print(\"\\ncategory: \",twenty_train.target_names[clf_naive_count.predict(test_example_count)[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a txt file and predict its class\n",
    "Classifing the (About the Research Topic) section on \"[computational neuroscience of deep brain simulation](http://journal.frontiersin.org/researchtopic/5705/computational-neuroscience-of-deep-brain-stimulation)\"  \n",
    "As expected both the models classify it in science (medician) category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "count_category:  sci.med\n",
      "\n",
      "tfidf_category:  sci.med\n"
     ]
    }
   ],
   "source": [
    "file = open('neuroscience.txt')\n",
    "file_data = file.read()\n",
    "file_data_count = count_vect.transform([file_data])\n",
    "file_data_tfidf = tfidf_transformer.transform(file_data_count)\n",
    "print(\"\\ncount_category: \",twenty_train.target_names[clf_naive_count.predict(file_data_count)[0]])\n",
    "print(\"\\ntfidf_category: \",twenty_train.target_names[clf_naive_tfidf.predict(file_data_tfidf)[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the performance on the test dataset\n",
    "As expected tfidf features gives some better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "count_accuracy: 0.772835900159\n",
      "\n",
      "tfidf_accuracy: 0.77389803505\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "naive_count_predicted = clf_naive_count.predict(X_test_counts)\n",
    "naive_tfidf_predicted = clf_naive_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "print(\"\\ncount_accuracy:\",np.mean(naive_count_predicted == twenty_test.target))\n",
    "print(\"\\ntfidf_accuracy:\",np.mean(naive_tfidf_predicted == twenty_test.target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(py3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
